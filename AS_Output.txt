23/04/03 13:09:13 INFO SparkContext: Running Spark version 3.3.2
23/04/03 13:09:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/04/03 13:09:13 INFO ResourceUtils: ==============================================================
23/04/03 13:09:13 INFO ResourceUtils: No custom resources configured for spark.driver.
23/04/03 13:09:13 INFO ResourceUtils: ==============================================================
23/04/03 13:09:13 INFO SparkContext: Submitted application: Assignment06
23/04/03 13:09:13 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/04/03 13:09:13 INFO ResourceProfile: Limiting resource is cpu
23/04/03 13:09:13 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/04/03 13:09:13 INFO SecurityManager: Changing view acls to: richardhoehn
23/04/03 13:09:13 INFO SecurityManager: Changing modify acls to: richardhoehn
23/04/03 13:09:13 INFO SecurityManager: Changing view acls groups to: 
23/04/03 13:09:13 INFO SecurityManager: Changing modify acls groups to: 
23/04/03 13:09:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(richardhoehn); groups with view permissions: Set(); users  with modify permissions: Set(richardhoehn); groups with modify permissions: Set()
23/04/03 13:09:13 INFO Utils: Successfully started service 'sparkDriver' on port 62426.
23/04/03 13:09:13 INFO SparkEnv: Registering MapOutputTracker
23/04/03 13:09:13 INFO SparkEnv: Registering BlockManagerMaster
23/04/03 13:09:13 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/04/03 13:09:13 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/04/03 13:09:13 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/04/03 13:09:13 INFO DiskBlockManager: Created local directory at /private/var/folders/8c/5_2vy2pn72x609w4xvq0yjzc0000gn/T/blockmgr-6375606d-555b-4943-b5f9-b451c1363198
23/04/03 13:09:13 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/04/03 13:09:13 INFO SparkEnv: Registering OutputCommitCoordinator
23/04/03 13:09:13 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/04/03 13:09:13 INFO Executor: Starting executor ID driver on host richards-mbp.localdomain
23/04/03 13:09:13 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/04/03 13:09:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62427.
23/04/03 13:09:13 INFO NettyBlockTransferService: Server created on richards-mbp.localdomain:62427
23/04/03 13:09:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/04/03 13:09:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, richards-mbp.localdomain, 62427, None)
23/04/03 13:09:13 INFO BlockManagerMasterEndpoint: Registering block manager richards-mbp.localdomain:62427 with 434.4 MiB RAM, BlockManagerId(driver, richards-mbp.localdomain, 62427, None)
23/04/03 13:09:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, richards-mbp.localdomain, 62427, None)
23/04/03 13:09:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, richards-mbp.localdomain, 62427, None)

*
Question Start: **************
********** rdd_data **********
['this is an example of spark', 'the example contains several lines', 'spark has low level and high level api']
Type:  <class 'pyspark.rdd.RDD'>
********** rdd_data **********
*

*
Question 1: ******************
********** rdd_split *********
['this', 'is', 'an', 'example', 'of', 'spark', 'the', 'example', 'contains', 'several', 'lines', 'spark', 'has', 'low', 'level', 'and', 'high', 'level', 'api']
Type:  <class 'pyspark.rdd.PipelinedRDD'>
********** rdd_split *********
*

*
Question 2: ******************
********** rdd_map ***********
[('this', 1), ('is', 1), ('an', 1), ('example', 1), ('of', 1), ('spark', 1), ('the', 1), ('example', 1), ('contains', 1), ('several', 1), ('lines', 1), ('spark', 1), ('has', 1), ('low', 1), ('level', 1), ('and', 1), ('high', 1), ('level', 1), ('api', 1)]
Type:  <class 'pyspark.rdd.PipelinedRDD'>
********** rdd_map ***********
*

*
Question 3: ******************
********** rdd_reduce ********
[('this', 1), ('is', 1), ('an', 1), ('example', 2), ('of', 1), ('spark', 2), ('the', 1), ('contains', 1), ('several', 1), ('lines', 1), ('has', 1), ('low', 1), ('level', 2), ('and', 1), ('high', 1), ('api', 1)]
Type:  <class 'pyspark.rdd.PipelinedRDD'>
********** rdd_reduce ********
*

*
Question 4: ******************
********** rdd_filter ********
[('spark', 2), ('several', 1)]
Type:  <class 'pyspark.rdd.PipelinedRDD'>
********** rdd_filter ********
*

*
Question 5: ******************
********** data **************
['r', 'e', 's', 'i', 'l', 'i', 'e', 'n', 't', ' ', 'd', 'i', 's', 't', 'r', 'i', 'b', 'u', 't', 'e', 'd', ' ', 'd', 'a', 't', 'a', 's', 'e', 't', 's']
Type:  <class 'pyspark.rdd.RDD'>
********** data **************
*

*
Question 6.0: ****************
********** distinctOut *******
['r', 's', 'i', 'l', 'd', 'b', 'e', 'n', 't', ' ', 'u', 'a']
Type:  <class 'pyspark.rdd.PipelinedRDD'>
********** distinctOut *******
*

*
Question 6.1: ****************
********** distinctOutCount **
12
Type:  <class 'int'>
********** distinctOutCount **
*

*
Question 7.0: ****************
********** dataMap ***********
[('r', 'r', False), ('e', 'e', False), ('s', 's', False), ('i', 'i', False), ('l', 'l', False), ('i', 'i', False), ('e', 'e', False), ('n', 'n', False), ('t', 't', False), (' ', ' ', False), ('d', 'd', True), ('i', 'i', False), ('s', 's', False), ('t', 't', False), ('r', 'r', False), ('i', 'i', False), ('b', 'b', False), ('u', 'u', False), ('t', 't', False), ('e', 'e', False), ('d', 'd', True), (' ', ' ', False), ('d', 'd', True), ('a', 'a', False), ('t', 't', False), ('a', 'a', False), ('s', 's', False), ('e', 'e', False), ('t', 't', False), ('s', 's', False)]
Type:  <class 'pyspark.rdd.PipelinedRDD'>
********** dataMap ***********
*

*
Question 7.1: ****************
********** dataMapFilter *****
[('d', 'd', True), ('d', 'd', True), ('d', 'd', True)]
Type:  <class 'list'>
********** dataMapFilter *****
*
